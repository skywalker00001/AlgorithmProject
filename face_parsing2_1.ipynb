{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_parsing2.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOqPAMiy1BL9O11XM3M2W9p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skywalker00001/AlgorithmProject/blob/master/face_parsing2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Version2.1"
      ],
      "metadata": {
        "id": "Eg9FYmTVucFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "pbKVFEytuozV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "ROOT = 'drive/MyDrive/ACV/Project1'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ha1h1YqSy7Ll",
        "outputId": "12e2d0e0-60c9-40d5-abc7-c62b7ad40dad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf_UbIBd-8PK",
        "outputId": "67c9daf6-85ab-4d3e-9fb3-86eee3ae5db5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Mar 22 22:17:54 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "!pip install wandb -qqq\n",
        "import wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GjvC0ODu4Le",
        "outputId": "20c7370a-72d7-4bad-d099-5443af0d97f7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: segmentation_models_pytorch in /usr/local/lib/python3.7/dist-packages (0.2.1)\n",
            "Requirement already satisfied: efficientnet-pytorch==0.6.3 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.6.3)\n",
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.4.12)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.11.1+cu111)\n",
            "Requirement already satisfied: pretrainedmodels==0.7.4 in /usr/local/lib/python3.7/dist-packages (from segmentation_models_pytorch) (0.7.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.63.0)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation_models_pytorch) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation_models_pytorch) (1.21.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to wandb to log the model run and all the parameters\n",
        "# 7229adacb32965027d73056a6927efd0365a00bc\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnHq35Rpu-Lh",
        "outputId": "29b1ac21-7602-479b-ef67-8aa67f7b0ec2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskywalk3r\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rc4LeCMsvBFu",
        "outputId": "2f56060b-bc5b-4df4-ee6b-e081666c38c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskywalk3r\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8_ZRhDNuZF_"
      },
      "outputs": [],
      "source": [
        "#import wandb\n",
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import time\n",
        "\n",
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "from torch import cuda\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import segmentation_models_pytorch as smp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "DEVICE = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(\"DEVICE is: \", DEVICE)\n",
        "\n",
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED) # pytorch random seed\n",
        "np.random.seed(SEED) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "JosMSrKbxPse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils"
      ],
      "metadata": {
        "id": "EHbrVUb-TP91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy2d(input, target, weight=None, size_average=True):\n",
        "    n, c, h, w = input.size()\n",
        "    nt, ht, wt = target.size()\n",
        "\n",
        "    # Handle inconsistent size between input and target\n",
        "    if h != ht or w != wt:\n",
        "        input = nn.functional.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
        "\n",
        "    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
        "    target = target.view(-1)\n",
        "    loss = nn.functional.cross_entropy(\n",
        "        input, target, weight=weight, size_average=size_average, ignore_index=250\n",
        "    )\n",
        "    # print('lo1 shape', loss.shape)\n",
        "    return loss\n",
        "\n",
        "# predict, groundtruth are both [batch, imsize, imsize]\n",
        "def get_miou(predict, groundtruth, num_classes=19, smoothing=1e-6):\n",
        "    pred = predict.to('cpu')\n",
        "    grdth = groundtruth.to('cpu')\n",
        "    miou_sum = 0\n",
        "    batch = predict.size()[0]\n",
        "    for idx in range(batch):\n",
        "        area_intersect_all = torch.zeros(num_classes).to('cpu')\n",
        "        area_union_all = torch.zeros(num_classes).to('cpu')\n",
        "        for cls_idx in range(num_classes):\n",
        "            area_intersect = torch.sum((pred[idx] == grdth[idx]) * (pred[idx] == cls_idx))\n",
        "            area_pred_label = torch.sum(pred[idx] == cls_idx)\n",
        "            area_gt_label = torch.sum(grdth[idx] == cls_idx)\n",
        "            area_union = area_pred_label + area_gt_label - area_intersect\n",
        "\n",
        "            area_intersect_all[cls_idx] += area_intersect + smoothing\n",
        "            area_union_all[cls_idx] += area_union + smoothing\n",
        "\n",
        "        iou_all = area_intersect_all / area_union_all * 100.0\n",
        "        miou = iou_all.mean()\n",
        "\n",
        "        miou_sum += miou\n",
        "\n",
        "    return miou_sum"
      ],
      "metadata": {
        "id": "SS1aL5SfTY7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_my_palette(impath):\n",
        "    img = Image.open(impath) \n",
        "    palette = img.getpalette()\n",
        "    return palette\n",
        "\n",
        "def put_my_palette(img, pale):\n",
        "    img = img.putpalette(pale)\n",
        "    return img\n",
        "\n",
        "# ts: [512, 512] after * 255\n",
        "def tensor2uint18(ts):\n",
        "    ts = ts.long().cpu().numpy()\n",
        "    ts = ts.astype(np.uint8)\n",
        "    return ts"
      ],
      "metadata": {
        "id": "amcsIwk9cJcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# images: [batch, 3, 512, 512], tensor\n",
        "# labels: [batch, 512, 512], tensor\n",
        "def wandb_log_image_table(images, predicted, labels, pale):\n",
        "    \"Log a wandb.Table with (img, pred, target, scores)\"\n",
        "    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"])\n",
        "    images, predicted, labels = images.cpu(), predicted.cpu(), labels.cpu()\n",
        "    #my_pale = \n",
        "    for img, pred, targ in zip(images, predicted, labels):\n",
        "        # img\n",
        "        img = img.long().numpy()\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "        # pred\n",
        "        pred = tensor2uint18(pred)\n",
        "        pred = Image.fromarray(pred)\n",
        "        pred.putpalette(pale)\n",
        "\n",
        "        # targ\n",
        "        targ = tensor2uint18(targ)\n",
        "        targ = Image.fromarray(targ)\n",
        "        targ.putpalette(pale)\n",
        "\n",
        "        # add_data\n",
        "        table.add_data(wandb.Image(img), wandb.Image(pred), wandb.Image(targ))\n",
        "    #wandb.log({\"predictions_table\":table}, commit=False)\n",
        "    wandb.log({\"predictions_table\":table}, commit=False)"
      ],
      "metadata": {
        "id": "5xoK4HMYDwjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, optimizer, epoch, miou, PATH):\n",
        "    torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epoch': epoch,\n",
        "            'miou': miou\n",
        "            }, PATH)\n",
        "# Helper function to print time \n",
        "def total_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs\n",
        "\n",
        "# def load_model(model, optimizer, PATH):\n",
        "#     checkpoint = torch.load(PATH)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "#     epoch = checkpoint['epoch']\n",
        "#     miou = checkpoint['miou']\n",
        "#     return model, optimizer, epoch, miou"
      ],
      "metadata": {
        "id": "ItlPUHGfp3cc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loader"
      ],
      "metadata": {
        "id": "zlv-jyRvHKko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "    mode=\"train/val/test\"\n",
        "    img.shape: torch.Size([3, 512, 512])\n",
        "    label.shape: torch.Size([1, 512, 512])\n",
        "'''\n",
        "class FaceParse_Dataset(Dataset):\n",
        "    def __init__(self, img_path, label_path, transform_img, transform_label, mode=\"train\"): \n",
        "        self.img_path = img_path\n",
        "        self.label_path = label_path\n",
        "        self.transform_img = transform_img\n",
        "        self.transform_label = transform_label\n",
        "        self.train_dataset = []\n",
        "        self.val_dataset = []\n",
        "        self.test_dataset = []\n",
        "        self.mode = mode\n",
        "        self.preprocess()\n",
        "        \n",
        "        if mode == \"train\":\n",
        "            self.num_images = len(self.train_dataset)\n",
        "        elif mode == \"val\":\n",
        "            self.num_images = len(self.val_dataset)\n",
        "        else :\n",
        "            self.num_images = len(self.test_dataset)\n",
        "\n",
        "    def preprocess(self):\n",
        "        \n",
        "        for i in range(len([name for name in os.listdir(self.img_path) if os.path.isfile(os.path.join(self.img_path, name))])):\n",
        "            img_path = os.path.join(self.img_path, str(i)+'.jpg')\n",
        "            # label_path = os.path.join(self.label_path, str(i)+'.png')\n",
        "            #print (img_path, label_path) \n",
        "            if self.mode == \"train\":\n",
        "                label_path = os.path.join(self.label_path, str(i)+'.png')\n",
        "                self.train_dataset.append([img_path, label_path])\n",
        "            elif self.mode == \"val\":\n",
        "                label_path = os.path.join(self.label_path, str(i)+'.png')\n",
        "                self.val_dataset.append([img_path, label_path])\n",
        "            elif self.mode == \"test\":\n",
        "                self.test_dataset.append(img_path)\n",
        "            \n",
        "        print(f'Finished preprocessing the CelebA dataset in {self.mode} mode...')\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.mode == \"test\":\n",
        "            dataset = self.test_dataset\n",
        "            img_path = dataset[index]\n",
        "            image = Image.open(img_path)\n",
        "            return self.transform_img(image)\n",
        "        else: \n",
        "            dataset = self.train_dataset if self.mode == \"train\" else self.val_dataset\n",
        "            img_path, label_path = dataset[index]\n",
        "            image = Image.open(img_path)\n",
        "            label = Image.open(label_path)\n",
        "            return self.transform_img(image), self.transform_label(label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of images.\"\"\"\n",
        "        return self.num_images\n",
        "\n",
        "class Data_Loader():\n",
        "    def __init__(self, img_path, label_path, image_size, batch_size, mode):\n",
        "        self.img_path = img_path\n",
        "        self.label_path = label_path\n",
        "        self.imsize = image_size\n",
        "        self.batch = batch_size\n",
        "        self.mode = mode\n",
        "\n",
        "    def transform_img(self, resize, totensor, normalize, centercrop):\n",
        "        options = []\n",
        "        if centercrop:\n",
        "            options.append(T.CenterCrop(160))\n",
        "        if resize:\n",
        "            options.append(T.Resize((self.imsize,self.imsize)))\n",
        "        if totensor:\n",
        "            options.append(T.ToTensor())\n",
        "        if normalize:\n",
        "            options.append(T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
        "        transform = T.Compose(options)\n",
        "        return transform\n",
        "\n",
        "    def transform_label(self, resize, totensor, normalize, centercrop):\n",
        "        options = []\n",
        "        if centercrop:\n",
        "            options.append(T.CenterCrop(160))\n",
        "        if resize:\n",
        "            options.append(T.Resize((self.imsize,self.imsize)))\n",
        "        if totensor:\n",
        "            options.append(T.ToTensor())\n",
        "        if normalize:\n",
        "            options.append(T.Normalize((0, 0, 0), (0, 0, 0)))\n",
        "        transform = T.Compose(options)\n",
        "        return transform\n",
        "\n",
        "    def loader(self):\n",
        "        transform_img = self.transform_img(True, True, True, False) \n",
        "        transform_label = self.transform_label(True, True, False, False)  \n",
        "        dataset = FaceParse_Dataset(self.img_path, self.label_path, transform_img, transform_label, self.mode)\n",
        "\n",
        "        loader = DataLoader(dataset=dataset,\n",
        "                            batch_size=self.batch,\n",
        "                            #shuffle=True,\n",
        "                            shuffle=(self.mode==\"train\"),\n",
        "                            num_workers=2,\n",
        "                            drop_last=False)\n",
        "        return loader"
      ],
      "metadata": {
        "id": "WW5T5isdvH_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer"
      ],
      "metadata": {
        "id": "eyFJWVzT0L5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, optimizer, train_loader, val_loader, config):\n",
        "\n",
        "        self.model_version = config[\"MODEL_VERSION\"]\n",
        "        self.device = config[\"DEVICE\"]\n",
        "        self.pale = config[\"PALETTE\"]\n",
        "\n",
        "        # Data loader\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "\n",
        "        # exact model and optimizer\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "        self.num_epoch = config[\"NUM_EPOCH\"]\n",
        "        self.start_epoch = config[\"START_EPOCH\"]\n",
        "        self.num_classes = config[\"NUM_CLASSES\"]\n",
        "\n",
        "        # Save model\n",
        "        self.model_save_step = config[\"MODEL_SAVE_STEP\"]\n",
        "        self.model_save_path = config[\"MODEL_SAVE_PATH\"]\n",
        "\n",
        "        self.best_mious = config[\"BEST_MIOUS\"]\n",
        "        self.best_epochs = config[\"BEST_EPOCHS\"]\n",
        "\n",
        "        self.need_log_images = config[\"LOG_IMAGES\"]\n",
        "        self.smoothing = config[\"SMOOTHING\"]\n",
        "    #     self.build_model\n",
        "\n",
        "    # def train(self):\n",
        "    #     self.model = \n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(self.start_epoch, self.start_epoch+ self.num_epoch):\n",
        "            # print(self.model)\n",
        "            # print(self.model.type)\n",
        "            self.model.train()\n",
        "\n",
        "            train_loss = 0\n",
        "            train_miou = 0\n",
        "            train_num = 0\n",
        "\n",
        "            val_loss = 0\n",
        "            val_miou = 0\n",
        "            val_num = 0\n",
        "\n",
        "            # train\n",
        "            with tqdm(total=len(self.train_loader), desc=\"training progress bar\") as progress_bar:\n",
        "                progress_bar.set_description('Epoch: {}/{} training'.format(epoch+1, self.start_epoch+ self.num_epoch ))\n",
        "                for batch, (imgs, labels) in enumerate(self.train_loader):\n",
        "                    # imgs: [batch, 3, imsize, imsize]\n",
        "                    # labels: [batch, 1, imsize, imsize]\n",
        "                    imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                    #print(labels)\n",
        "\n",
        "                    # Forward\n",
        "                    # outputs: [batch, num_class, imsize, imsize]\n",
        "                    outputs = self.model(imgs)\n",
        "\n",
        "                    # *255 to restore the real pixel value (transform.ToTensor has / 255)\n",
        "                    labels[:, 0, :, :] = labels[:, 0, :, :] * 255.0\n",
        "                    # labels_real_plain: [batch, imsize, imsize]\n",
        "                    labels_real_plain = labels[:, 0, :, :]\n",
        "                    # print(labels_real_plain)\n",
        "\n",
        "                    # compute loss\n",
        "                    # print('outputs shape: ', outputs.shape)\n",
        "                    # print('labels_real_plain shape: ', labels_real_plain.long().shape)\n",
        "                    loss = cross_entropy2d(outputs, labels_real_plain.long())\n",
        "                    # print('loss shape: ', loss.shape)\n",
        "                    # print('imgs.size(0): ', imgs.size(0))\n",
        "                    # print('loss.data:', loss.data)\n",
        "                    train_loss += loss.data * imgs.size(0)\n",
        "                    # Backprop the gradient and update parameters\n",
        "                    self.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                    # compute miou\n",
        "                    # pred_mask: [batch, imsize, imsize]\n",
        "                    pred_mask = torch.argmax(outputs, dim=1)\n",
        "                    train_miou += get_miou(pred_mask, labels_real_plain, num_classes=self.num_classes, smoothing=self.smoothing)\n",
        "                    # count total_num\n",
        "                    train_num += imgs.size(0)\n",
        "\n",
        "                    if(batch) % 10 == 9:\n",
        "                        progress_bar.set_postfix(batch='{}'.format(batch),\n",
        "                                                 train_miou=\"{:.2f}%\".format(train_miou / train_num),\n",
        "                                                 train_loss='{:.5f}'.format(train_loss / train_num))\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "            # update wandb\n",
        "            wandb.log({\"train_loss\": (train_loss / train_num), \"train_miou\": (train_miou / train_num), \\\n",
        "                        \"epoch\": epoch+1}, step = epoch - self.start_epoch)\n",
        "            # Log validation metrics\n",
        "            # Needchange\n",
        "            val_loss, val_miou = self.valid(epoch, log_images=self.need_log_images, batch_idx=0)\n",
        "\n",
        "            wandb.log({\"val_loss\": val_loss, \"val_miou\": val_miou}, step=epoch - self.start_epoch)\n",
        "            #print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, accuracy: {accuracy:.2f}\")\n",
        "        \n",
        "            # Save model based on miou\n",
        "            min_best_mious = min(self.best_mious)\n",
        "            indexof_min_best_mious = self.best_mious.index(min(self.best_mious))\n",
        "            if val_miou > min_best_mious:   # replace the model which has the min miou with current model.\n",
        "                self.best_mious[indexof_min_best_mious] = val_miou\n",
        "                self.best_epochs[indexof_min_best_mious] = epoch\n",
        "                save_model(self.model, self.optimizer, epoch+1, val_miou, \\\n",
        "                           os.path.join(self.model_save_path, '{}_MODEL{}.pth').format(indexof_min_best_mious, self.model_version))\n",
        "                \n",
        "                #os.path.join(self.model_save_path, '{}_MODEL.pth'.format(epoch + 1)))\n",
        "                print(\"Saving best model at epoch {} with miou {}\".format(epoch+1, val_miou))\n",
        "            #            os.path.join(self.model_save_path, '{}_G.pth'.format(step + 1)))\n",
        "\n",
        "            # Save model based on epoch\n",
        "            #Needchange\n",
        "            if epoch % self.model_save_step == (self.model_save_step - 1):   # each 10 epochs save the model once.\n",
        "\n",
        "                save_model(self.model, self.optimizer, epoch+1, val_miou, \\\n",
        "                           os.path.join(self.model_save_path, 'FUNDAMODEL{}.pth').format(self.model_version))\n",
        "                \n",
        "                #os.path.join(self.model_save_path, '{}_MODEL.pth'.format(epoch + 1)))\n",
        "                print(\"Saving fundamodel at epoch {} with miou {}\".format(epoch+1, val_miou))\n",
        "            #            os.path.join(self.model_save_path, '{}_G.pth'.format(step + 1)))   \n",
        "\n",
        "        return self.best_mious ,self.best_epochs\n",
        "\n",
        "    # def load_pretrained_model(self):\n",
        "    #     self.G.load_state_dict(torch.load(os.path.join(\n",
        "    #         self.model_save_path, '{}_G.pth'.format(self.pretrained_model))))\n",
        "    #     print('loaded trained models (step: {})..!'.format(self.pretrained_model))\n",
        "\n",
        "    def valid(self, epoch, log_images=False, batch_idx=0):\n",
        "        \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
        "        self.model.eval()\n",
        "        val_loss = 0.\n",
        "        val_miou = 0.\n",
        "        val_num = 0\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            with tqdm(total=len(val_loader), desc=\"validating progress bar\") as progress_bar:\n",
        "                progress_bar.set_description('epoch: {}/{} validating'.format(epoch+1, self.start_epoch+ self.num_epoch))\n",
        "\n",
        "                for batch, (imgs, labels) in enumerate(self.val_loader):\n",
        "                    # imgs: [batch, 3, imsize, imsize]\n",
        "                    # labels: [batch, 1, imsize, imsize]\n",
        "                    imgs, labels = imgs.to(self.device), labels.to(self.device)\n",
        "                    # outputs: [batch, num_class, imsize, imsize]\n",
        "                    outputs = self.model(imgs)\n",
        "                    # *255 to restore the real pixel value (transform.ToTensor has / 255)\n",
        "                    labels[:, 0, :, :] = labels[:, 0, :, :] * 255.0\n",
        "                    # labels_real_plain: [batch, imsize, imsize]\n",
        "                    labels_real_plain = labels[:, 0, :, :]\n",
        "                    # compute loss (average loss over one batch)\n",
        "                    loss = cross_entropy2d(outputs, labels_real_plain.long())\n",
        "                    val_loss += loss.data * imgs.size(0)\n",
        "                    # compute miou\n",
        "                    # pred_mask: [batch, imsize, imsize]\n",
        "                    pred_mask = torch.argmax(outputs, dim=1)\n",
        "                    val_miou += get_miou(pred_mask, labels_real_plain, num_classes=self.num_classes, smoothing=self.smoothing)\n",
        "                    # count val_num\n",
        "                    val_num += imgs.size(0)\n",
        "\n",
        "                    # Log one batch of images to the dashboard, always same batch_idx.\n",
        "                    # Needchange\n",
        "                    #if batch==batch_idx and log_images:\n",
        "                    if (epoch % self.model_save_step == (self.model_save_step - 1)) and batch==batch_idx and log_images:\n",
        "                        wandb_log_image_table(imgs*255, pred_mask, labels_real_plain, pale=self.pale)\n",
        "                    # update progress_bar\n",
        "                    progress_bar.set_postfix(miou=\"{:.2f}%\".format(val_miou / val_num),\n",
        "                                              loss='{:.5f}'.format(val_loss / val_num))\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "        return val_loss / val_num, val_miou / val_num\n",
        "\n"
      ],
      "metadata": {
        "id": "vdesqO4u7Qm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tester"
      ],
      "metadata": {
        "id": "FPqFKSiyZQZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Tester(object):\n",
        "    def __init__(self, model, test_loader, config):\n",
        "\n",
        "        #self.model_version = config[\"MODEL_VERSION\"]\n",
        "        self.device = config[\"DEVICE\"]\n",
        "        self.pale = config[\"PALETTE\"]\n",
        "\n",
        "        # exact model and optimizer\n",
        "        self.model = model\n",
        "        # Data loader\n",
        "        self.test_loader = test_loader\n",
        "\n",
        "        # self.optimizer = optimizer\n",
        "        # self.imsize = config[\"IMSIZE\"]\n",
        "        # self.num_classes = config[\"NUM_CLASSES\"]\n",
        "        self.test_batch_size = config[\"TEST_BATCH_SIZE\"]\n",
        "        self.gray_label_path = os.path.join(config[\"RESULTS_PATH\"], 'gray')\n",
        "        self.color_label_path = os.path.join(config[\"RESULTS_PATH\"], 'color')\n",
        "\n",
        "        self.making_files()\n",
        "\n",
        "        # # Model hyper-parameters\n",
        "        # self.imsize = config[\"IMSIZE\"]\n",
        "        # self.train_batch_size = config[\"TRAIN_BATCH_SIZE\"]\n",
        "        # self.val_batch_size = config[\"VAL_BATCH_SIZE\"]\n",
        "        # self.num_workers = config.num_workers\n",
        "        # self.g_lr = config[\"LEARNING_RATE\"]\n",
        "        # self.lr_decay = config[\"LR_DECAY\"]\n",
        "        # self.beta1 = config[\"BETA1\"]\n",
        "        # self.beta2 = config[\"BETA2\"]\n",
        "        # self.model = config[\"MODEL\"]\n",
        "        # self.num_epoch = config[\"NUM_EPOCH\"]\n",
        "        # self.start_epoch = config[\"START_EPOCH\"]\n",
        "\n",
        "        # # Save model\n",
        "        # self.model_save_step = config[\"MODEL_SAVE_STEP\"]\n",
        "        # self.model_save_path = config[\"MODEL_SAVE_PATH\"]\n",
        "\n",
        "        # self.best_miou = config[\"BEST_MIOU\"]\n",
        "        # self.best_epoch = config[\"BEST_EPOCH\"]\n",
        "\n",
        "\n",
        "    # def test2(self):\n",
        "    #     self.model.eval() \n",
        "    #     batch_num = int(self.test_size / self.batch_size)\n",
        "\n",
        "    #     for i in range(batch_num):\n",
        "    #         print (i)\n",
        "    #         imgs = []\n",
        "    #         for j in range(self.batch_size):\n",
        "    #             path = test_paths[i * self.batch_size + j]\n",
        "    #             img = transform(Image.open(path))\n",
        "    #             imgs.append(img)\n",
        "    #         imgs = torch.stack(imgs) \n",
        "    #         imgs = imgs.cuda()\n",
        "    #         labels_predict = self.G(imgs)\n",
        "    #         labels_predict_plain = generate_label_plain(labels_predict)\n",
        "    #         labels_predict_color = generate_label(labels_predict)\n",
        "    #         for k in range(self.batch_size):\n",
        "    #             cv2.imwrite(os.path.join(self.test_label_path, str(i * self.batch_size + k) +'.png'), labels_predict_plain[k])\n",
        "    #             save_image(labels_predict_color[k], os.path.join(self.test_color_label_path, str(i * self.batch_size + k) +'.png'))\n",
        "    def making_files(self):\n",
        "        if not os.path.exists(self.gray_label_path):\n",
        "              os.mkdir(self.gray_label_path)\n",
        "              print(\"New gray file!\")\n",
        "        if not os.path.exists(self.color_label_path):\n",
        "              os.mkdir(self.color_label_path)\n",
        "              print(\"New color file!\")\n",
        "\n",
        "    def test(self):\n",
        "        \"Compute performance of the model on the test dataset.\"\n",
        "        self.model.eval()\n",
        "  \n",
        "        # test_loss = 0.\n",
        "        # test_miou = 0.\n",
        "        test_num = 0\n",
        "        result_num = []\n",
        "\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            for batch, imgs in enumerate(self.test_loader):\n",
        "                # imgs: [batch, 3, imsize, imsize]\n",
        "                # labels: [batch, 1, imsize, imsize]\n",
        "                imgs = imgs.to(self.device)\n",
        "                # outputs: [batch, num_class, imsize, imsize]\n",
        "                outputs = self.model(imgs)\n",
        "                # *255 to restore the real pixel value (transform.ToTensor has / 255)\n",
        "                # labels[:, 0, :, :] = labels[:, 0, :, :] * 255.0\n",
        "                # labels_real_plain: [batch, imsize, imsize]\n",
        "                # labels_real_plain = labels[:, 0, :, :]\n",
        "                # compute loss (average loss over one batch)\n",
        "                # loss = cross_entropy2d(outputs, labels_real_plain.long())\n",
        "                # test_loss += loss.data * imgs.size(0)\n",
        "                # compute miou\n",
        "                # pred_mask_tensor: [batch, imsize, imsize]\n",
        "                pred_mask_tensor = torch.argmax(outputs, dim=1)\n",
        "                pred_mask_numpy = pred_mask_tensor.cpu().numpy()\n",
        "                # test_miou += get_miou(pred_mask, labels_real_plain, num_classes=self.num_classes)\n",
        "                # print(\"size: \", pred_mask_tensor.shape)\n",
        "                for k in range(imgs.size(0)):\n",
        "                    # gray_piciture\n",
        "                    cv2.imwrite(os.path.join(self.gray_label_path, str(test_num + k) +'.png'), pred_mask_numpy[k])\n",
        "                    # print(\"Gray pic {}.png\".format(test_num + k))\n",
        "\n",
        "                    # color_picture\n",
        "                    color_label = tensor2uint18(pred_mask_tensor[k])\n",
        "                    color_label = Image.fromarray(color_label)\n",
        "                    color_label.putpalette(self.pale)\n",
        "                    color_label.save(os.path.join(self.color_label_path, str(test_num + k) +'.png'))\n",
        "\n",
        "                    result_num.append(test_num + k)\n",
        "                    # print(\"Color pic {}.png\".format(test_num + k))\n",
        "                    #cv2.imwrite(os.path.join(self.gray_label_path, str(test_num + k) +'.png'), pred_mask[k])\n",
        "\n",
        "                # count test_num\n",
        "                test_num += imgs.size(0)\n",
        "                    # save_image(labels_predict_color[k], os.path.join(self.test_color_label_path, str(i * self.batch_size + k) +'.png'))\n",
        "                # Log one batch of images to the dashboard, always same batch_idx.\n",
        "                # if epoch % 10 == 0 and batch==batch_idx and log_images:\n",
        "                #     wandb_log_image_table(imgs*255, pred_mask, labels_real_plain, pale=self.pale)\n",
        "        \n",
        "        print(\"Testing {} results has completed!\".format(test_num))\n",
        "        return result_num\n",
        "        # return test_loss / test_num, test_miou / test_num\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7r9WI2mffsie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "Q83rGpkkvMet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Parameters = {\n",
        "    \"MODEL_VERSION\": '2.1',\n",
        "    \"MODEL_LOAD_VERSION\": '2.1',\n",
        "    \"DEVICE\": DEVICE,\n",
        "\n",
        "    # Needchange\n",
        "    \"NUM_EPOCH\": 40,\n",
        "    \"START_EPOCH\": 0,\n",
        "    \"NUM_CLASSES\": 19,\n",
        "    \"IMSIZE\": 512,\n",
        "    \"TRAIN_BATCH_SIZE\": 16,    # input batch size for training (default: 64)\n",
        "    \"VAL_BATCH_SIZE\": 64,    # input batch size for testing (default: 1000)\n",
        "    \"TEST_BATCH_SIZE\": 64, \n",
        "    #\"TRAIN_EPOCHS\": 51,        # number of epochs to train (default: 10)\n",
        "    #\"SEED\": 42,               # random seed (default: 42)\n",
        "\n",
        "    # Model Para\n",
        "    \"LEARNING_RATE\": 1e-4 ,   # learning rate (default: 0.01)\n",
        "    \"LR_DECAY\": 0.95,\n",
        "    \"BETA1\": 0.5,\n",
        "    \"BETA2\": 0.999,\n",
        "\n",
        "    # Path\n",
        "    \"TRAIN_PATH\": os.path.join(ROOT, 'train'),\n",
        "    \"VAL_PATH\": os.path.join(ROOT, 'val'),\n",
        "    \"TEST_PATH\": os.path.join(ROOT, 'test'),\n",
        "    \"RESULTS_PATH\": os.path.join(ROOT, 'results'),\n",
        "\n",
        "    # Save\n",
        "    \"MODEL_SAVE_STEP\": 5,\n",
        "    \"MODEL_SAVE_PATH\": os.path.join(ROOT, 'models'),\n",
        "\n",
        "    # Load\n",
        "    \"MODEL_IF_LOAD\": False,\n",
        "    \"MODEL_LOAD_PATH\": os.path.join(ROOT, 'models'),\n",
        "\n",
        "    # Best model\n",
        "    # \"BEST_MIOUS\": 5*[0],\n",
        "    # \"BEST_EPOCHS\": 5*[0],\n",
        "    \"EXPECTED_MODEL_NUMBER\": 4,\n",
        "\n",
        "    \"LOG_IMAGES\": False,\n",
        "    \"SMOOTHING\": 1e-6,\n",
        "}\n",
        "\n",
        "Parameters[\"BEST_MIOUS\"] = Parameters[\"EXPECTED_MODEL_NUMBER\"] * [0]\n",
        "Parameters[\"BEST_EPOCHS\"] = Parameters[\"EXPECTED_MODEL_NUMBER\"] * [0]\n",
        "# get palette\n",
        "Parameters[\"PALETTE\"] = get_my_palette(os.path.join(Parameters[\"TRAIN_PATH\"], 'train_mask/1.png'))\n",
        "# # Define model and optimizer\n",
        "# Parameters[\"MODEL\"] = smp.Unet(\n",
        "#         encoder_name=\"resnet34\",\n",
        "#         encoder_weights=\"imagenet\",\n",
        "#         in_channels=3,\n",
        "#         classes=Parameters[\"NUM_CLASSES\"],\n",
        "#     ).to(Parameters[\"DEVICE\"])\n",
        "\n",
        "# Parameters[\"OPTIMIZER\"] = torch.optim.Adam(filter(lambda p: p.requires_grad, Parameters[\"MODEL\"].parameters()), \\\n",
        "#                               Parameters[\"LEARNING_RATE\"], [Parameters[\"BETA1\"], Parameters[\"BETA2\"]])"
      ],
      "metadata": {
        "id": "qrHqHvBY9wic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_img_path = os.path.join(Parameters[\"TRAIN_PATH\"], 'train_image')\n",
        "train_label_path = os.path.join(Parameters[\"TRAIN_PATH\"], 'train_mask')\n",
        "val_img_path = os.path.join(Parameters[\"VAL_PATH\"], 'val_image')\n",
        "val_label_path = os.path.join(Parameters[\"VAL_PATH\"], 'val_mask')\n",
        "test_img_path = os.path.join(Parameters[\"TEST_PATH\"], 'test_image')"
      ],
      "metadata": {
        "id": "CjJ-JY_iHNWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_img_path = os.path.join(Parameters[\"TRAIN_PATH\"], 'train_image')\n",
        "# train_label_path = os.path.join(Parameters[\"TRAIN_PATH\"], 'train_mask')\n",
        "# val_img_path = os.path.join(Parameters[\"VAL_PATH\"], 'val_image')\n",
        "# val_label_path = os.path.join(Parameters[\"VAL_PATH\"], 'val_mask')\n",
        "# test_img_path = os.path.join(Parameters[\"TEST_PATH\"], 'test_image')\n",
        "\n",
        "# train_loader: img([8, 3, 512, 512]), label([8, 1, 512, 512])\n",
        "train_loader = Data_Loader(train_img_path, train_label_path, \\\n",
        "              Parameters[\"IMSIZE\"], Parameters[\"TRAIN_BATCH_SIZE\"], \"train\").loader()\n",
        "val_loader = Data_Loader(val_img_path, val_label_path, \\\n",
        "              Parameters[\"IMSIZE\"], Parameters[\"VAL_BATCH_SIZE\"], \"val\").loader()\n",
        "test_loader = Data_Loader(test_img_path, None, \\\n",
        "              Parameters[\"IMSIZE\"], Parameters[\"TEST_BATCH_SIZE\"], \"test\").loader()"
      ],
      "metadata": {
        "id": "2skOLdITxM1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "nQDd2jF-n45J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print()\n",
        "# print(\"loaded_best_mious: \" + str(Parameters[\"BEST_MIOUS\"]))\n",
        "# print(\"loaded_best_epochs: \" + str(Parameters[\"BEST_EPOCHS\"]))\n"
      ],
      "metadata": {
        "id": "Rs2_ddJMj4f1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with wandb.init(\n",
        "    # config = Parameters,\n",
        "    project=\"Face_Parsing\"+ Parameters[\"MODEL_VERSION\"],\n",
        "    ):\n",
        "    # config = wandb.config\n",
        "\n",
        "    # Define model and optimizer\n",
        "    model = smp.Unet(\n",
        "            encoder_name=\"resnet34\",\n",
        "            encoder_weights=\"imagenet\",\n",
        "            in_channels=3,\n",
        "            classes=Parameters[\"NUM_CLASSES\"],\n",
        "        ).to(Parameters[\"DEVICE\"])\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \\\n",
        "                                 Parameters[\"LEARNING_RATE\"], [Parameters[\"BETA1\"], Parameters[\"BETA2\"]])\n",
        "    start_time = time.time()\n",
        "\n",
        "    # if need, load the paramters of the model\n",
        "    if (Parameters[\"MODEL_IF_LOAD\"]):      \n",
        "        # update stored best_mious and best_epochs\n",
        "        num = Parameters[\"EXPECTED_MODEL_NUMBER\"]\n",
        "        b_mious, b_epochs = [], []\n",
        "        for i in range(num):\n",
        "            PATH = os.path.join(Parameters[\"MODEL_LOAD_PATH\"], \\\n",
        "                                                  \"{}_MODEL{}.pth\".format(i, Parameters[\"MODEL_LOAD_VERSION\"]))\n",
        "            checkpoint = torch.load(PATH)\n",
        "            # loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            b_epochs.append(checkpoint['epoch'])\n",
        "            b_mious.append(checkpoint['miou'])\n",
        "        Parameters[\"BEST_MIOUS\"] = b_mious\n",
        "        Parameters[\"BEST_EPOCHS\"] = b_epochs\n",
        "\n",
        "        print()\n",
        "        print(\"loaded_best_mious: \" + str(Parameters[\"BEST_MIOUS\"]))\n",
        "        print(\"loaded_best_epochs: \" + str(Parameters[\"BEST_EPOCHS\"]))\n",
        "        \n",
        "        # really load the model and optimizer\n",
        "        LOAD_PATH = os.path.join(Parameters[\"MODEL_LOAD_PATH\"], \\\n",
        "                                             \"FUNDAMODEL{}.pth\".format(Parameters[\"MODEL_LOAD_VERSION\"]))\n",
        "        load_checkpoint = torch.load(LOAD_PATH)\n",
        "        model.load_state_dict(load_checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(load_checkpoint['optimizer_state_dict'])\n",
        "        Parameters[\"START_EPOCH\"] = load_checkpoint['epoch']\n",
        "        print()\n",
        "        print('Now the model is at epoch {} with miou {}.'.format(load_checkpoint['epoch'], load_checkpoint['miou']))\n",
        "        # epoch = checkpoint['epoch']\n",
        "        # miou = checkpoint['miou']\n",
        "        # Parameters[\"BEST_EPOCH\"] = checkpoint['epoch']\n",
        "        # Parameters[\"BEST_MIOU\"] = checkpoint['miou']\n",
        "\n",
        "\n",
        "    trainer = Trainer(model, optimizer, train_loader, val_loader, Parameters)\n",
        "    result_best_mious, result_best_epochs = trainer.train()\n",
        "    print()\n",
        "    print(\"best_mious: \" + str(result_best_mious))\n",
        "    print(\"best_epochs: \" + str(result_best_epochs))\n",
        "    \n",
        "    end_time = time.time()\n",
        "    epoch_mins, epoch_secs = total_time(start_time, end_time)\n",
        "    print()\n",
        "    print(f'Total Time: {epoch_mins}m {epoch_secs}s')     \n",
        "    #trainer = Trainer(data_loader.loader(), config)\n",
        "    #trainer.train()"
      ],
      "metadata": {
        "id": "EcZFdsci-mQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Parameters[\"BEST_EPOCHS\"])\n",
        "print(Parameters['BEST_MIOUS'])"
      ],
      "metadata": {
        "id": "YxSGUL8_5tr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "iKE-AO41PMtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model and optimizer\n",
        "loaded_model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=Parameters[\"NUM_CLASSES\"],\n",
        "    ).to(Parameters[\"DEVICE\"])\n",
        "\n",
        "\n",
        "epochs, mious = [], []\n",
        "num = Parameters[\"EXPECTED_MODEL_NUMBER\"]\n",
        "for i in range(num):\n",
        "    PATH = os.path.join(Parameters[\"MODEL_LOAD_PATH\"], \\\n",
        "                                          \"{}_MODEL{}.pth\".format(i, Parameters[\"MODEL_LOAD_VERSION\"]))\n",
        "    checkpoint = torch.load(PATH)\n",
        "    # loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    epochs.append(checkpoint['epoch'])\n",
        "    mious.append(checkpoint['miou'])\n",
        "print('best_epochs: ' + str(epochs))\n",
        "print('best_mious: ' + str(mious))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n84scWYI4VP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOAD_PATH = os.path.join(Parameters[\"MODEL_LOAD_PATH\"], \\\n",
        "                                      \"{}_MODEL{}.pth\".format(1, Parameters[\"MODEL_LOAD_VERSION\"]))\n",
        "load_checkpoint = torch.load(LOAD_PATH)\n",
        "loaded_model.load_state_dict(load_checkpoint['model_state_dict'])\n",
        "loaded_epoch = load_checkpoint['epoch']\n",
        "loaded_miou = load_checkpoint['miou']\n",
        "print('loaded_epoch: ' + str(loaded_epoch))\n",
        "print('loaded_miou: ' + str(loaded_miou))\n",
        "\n",
        "\n",
        "tester = Tester(loaded_model, test_loader, Parameters)\n",
        "result_list = tester.test()\n"
      ],
      "metadata": {
        "id": "_x6QzpErBq78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_file = open(os.path.join(Parameters[\"RESULTS_PATH\"], 'result_number.txt'),'w+')\n",
        "result_file.write('total_number: ' + str(len(result_list)))\n",
        "result_file.write('\\n')\n",
        "result_file.write(str(result_list))\n",
        "result_file.close()"
      ],
      "metadata": {
        "id": "QGrtEWfYfOc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}